{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bafc21a2",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "8511556bcc499109a9a53a1af310f036",
          "grade": false,
          "grade_id": "cell-509ca0ded5c64f20",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "bafc21a2"
      },
      "source": [
        "# Assignment 2b: Word2Vec Representations (10 Marks)\n",
        "\n",
        "## Due: March 17, 2022\n",
        "\n",
        "Welcome to the Assignment 2b of the course. This week we will learn about vector representations for words and how can we utilize them to solve the sentiment analysis task that we discussed in the previous Assignment."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d8c70cb4",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d8c70cb4",
        "outputId": "fc1a6129-c5e3-46b1-a0b7-217d1584bc86"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "try:\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    data_dir = \"/content/gdrive/MyDrive/Colab Notebooks/PlakshaNLP/Assignment2b/data/SST-2\"\n",
        "except:\n",
        "    data_dir = \"/datadrive/t-kabir/work/repos/PlakshaNLP/source/Assignment2b/data/SST-2\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40186d50",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c4025dd1a18667a132e8af5e7b47e2c7",
          "grade": false,
          "grade_id": "cell-5fd0f300224e0638",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "40186d50",
        "outputId": "7292da4f-6a7a-41e6-a518-bf5f7a164c9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.21.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.3.5)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.7/dist-packages (3.2.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from nltk) (1.15.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.10.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.63.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.21.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (3.0.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.11.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (0.11.2)\n",
            "Requirement already satisfied: scipy>=1.0 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.4.1)\n",
            "Requirement already satisfied: pandas>=0.23 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.3.5)\n",
            "Requirement already satisfied: matplotlib>=2.2 in /usr/local/lib/python3.7/dist-packages (from seaborn) (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.15 in /usr/local/lib/python3.7/dist-packages (from seaborn) (1.21.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (2.8.2)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (3.0.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib>=2.2->seaborn) (0.11.0)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas>=0.23->seaborn) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.1->matplotlib>=2.2->seaborn) (1.15.0)\n",
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.2.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.21.5)\n"
          ]
        }
      ],
      "source": [
        "# Install required libraries\n",
        "!pip install numpy\n",
        "!pip install pandas\n",
        "!pip install nltk\n",
        "!pip install torch\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n",
        "!pip install seaborn\n",
        "!pip install gensim"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f21676ea",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "4b957d06ab9522ddb33fa0fd9da2565a",
          "grade": false,
          "grade_id": "cell-29786c43c3f990f2",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f21676ea",
        "outputId": "12f8b0db-3e4d-49eb-b064-6ed625385cad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# We start by importing libraries that we will be making use of in the assignment.\n",
        "import string\n",
        "import tqdm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import gensim\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import nltk\n",
        "nltk.download(\"punkt\")\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4f5845fb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6156768df682ba1f820d4aa703312da6",
          "grade": false,
          "grade_id": "cell-5de65c2212dbb4c4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4f5845fb"
      },
      "source": [
        "Similar to last time we will again be working on the Stanford Sentiment Dataset. Below we load the dataset into the memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a6828c92",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ddeaba17f88782257dc796a35c4ae0de",
          "grade": false,
          "grade_id": "cell-3b092d562863ae9c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6828c92",
        "outputId": "713bf0c1-5510-404e-89bf-94bc810202de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of Training Examples: 67349\n",
            "Number of Test Examples: 872\n"
          ]
        }
      ],
      "source": [
        "# We can use pandas to load the datasets\n",
        "train_df = pd.read_csv(f\"{data_dir}/train.tsv\", sep = \"\\t\")\n",
        "test_df = pd.read_csv(f\"{data_dir}/dev.tsv\", sep = \"\\t\")\n",
        "\n",
        "print(f\"Number of Training Examples: {len(train_df)}\")\n",
        "print(f\"Number of Test Examples: {len(test_df)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "998e30c1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "874e5314e53c05c8cdf13df2d3926197",
          "grade": false,
          "grade_id": "cell-e4595f3246a0e463",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "998e30c1",
        "outputId": "bf873b2f-1817-481e-8952-f4e9020503b4"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                            sentence  label\n",
              "0       hide new secretions from the parental units       0\n",
              "1               contains no wit , only labored gags       0\n",
              "2  that loves its characters and communicates som...      1\n",
              "3  remains utterly satisfied to remain the same t...      0\n",
              "4  on the worst revenge-of-the-nerds clichés the ...      0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-42ce2bfb-a9c3-46be-8cca-766dc5f57f36\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>hide new secretions from the parental units</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>contains no wit , only labored gags</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>that loves its characters and communicates som...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>remains utterly satisfied to remain the same t...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>on the worst revenge-of-the-nerds clichés the ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-42ce2bfb-a9c3-46be-8cca-766dc5f57f36')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-42ce2bfb-a9c3-46be-8cca-766dc5f57f36 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-42ce2bfb-a9c3-46be-8cca-766dc5f57f36');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "# View a sample of the dataset\n",
        "train_df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "328480bc",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0446c0c696b71c14bc7646e2c7acff02",
          "grade": false,
          "grade_id": "cell-6ed2b7d10d7204ea",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "328480bc"
      },
      "source": [
        "## Task 0: Warm Up Excercise (2 Marks)\n",
        "\n",
        "To start we ask you to re-implement some functions from the Assignment 1. Mainly you will implement the preprocessing pipeline and vocabulary building functions again as well as some new but related functions. Details about the functions will be given in their Doc Strings."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b03bcb88",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "f5bf4e61fdfc77d4b99ae4c185b19c83",
          "grade": false,
          "grade_id": "cell-979e1e262f589562",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "b03bcb88"
      },
      "source": [
        "### Task 0.1: Preprocessing Pipeline (1 Mark)\n",
        "\n",
        "Implement the preprocessing pipeline like we did in the previous assignment, however, this time we will only implement converting the text to lower case and removing punctuations.\n",
        "\n",
        "We are not doing any stemming this time as we will be using pre-trained word representations in this assignment, and like it was discussed in the lectures stemming often results in the words that may not exist in common dictionaries.\n",
        "\n",
        "We are also skipping stop words removal this time around, the reason is that removing stop words can often hurt the structural integrity of a sentence and the choice of stop words to use can be very subjective and depend upon the task at hand. For example: In the stop words list that we used last time also contained the word `not`, removing which can change the sentiment of the sentence, eg. I did not like this movie -> I did like this movie. In this assignment we will explore more sophisticated ways to handle the stop words than just directly removing them from the text."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8342b357",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "93a6b0d61b9ba2491a3b15cc831a1477",
          "grade": false,
          "grade_id": "cell-353290c941bcd294",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "8342b357"
      },
      "outputs": [],
      "source": [
        "def preprocess_pipeline(text):\n",
        "    \"\"\"\n",
        "    Given a piece of text applies preprocessing techniques\n",
        "    like converting to lower case, removing stop words and punctuations.\n",
        "\n",
        "    Apply the functions in the following order:\n",
        "    1. to_lower_case\n",
        "    2. remove_punctuations\n",
        "\n",
        "    Inputs:\n",
        "    - text (str) : A python string containing text to be pre-processed\n",
        "\n",
        "    Returns:\n",
        "    - text_preprocessed (str) : Resulting string after applying preprocessing\n",
        "    \n",
        "    Note: You may implement the functions for the two steps seperately in this cell\n",
        "            or just write all the code in this function only we leave that up to you.\n",
        "    \"\"\"\n",
        "    import string\n",
        "    text_preprocessed = text.lower()\n",
        "    text_preprocessed=text_preprocessed.translate(text_preprocessed.maketrans('','',string.punctuation))\n",
        "    return text_preprocessed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c8c53fd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a7aff55eabd7b591216cde8479f69345",
          "grade": true,
          "grade_id": "cell-f9c3aa14c0041549",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2c8c53fd",
        "outputId": "fe75ae14-0cf4-4b10-b6b5-030dd58b7f8f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1:\n",
            "Input: Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal!\n",
            "Function Output: mr and mrs dursley of number four privet drive were proud to say that they were perfectly normal\n",
            "Expected Output: mr and mrs dursley of number four privet drive were proud to say that they were perfectly normal\n",
            "Test Case Passed :)\n",
            "**********************************\n",
            "\n",
            "Sample Test Case 2:\n",
            "Input: \"Little tyke,\" chortled Mr. Dursley as He left the house.\n",
            "Function Output: little tyke chortled mr dursley as he left the house\n",
            "Expected Output: little tyke chortled mr dursley as he left the house\n",
            "Test Case Passed :)\n",
            "**********************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "\n",
        "def evaluate_string_test_cases(test_case_input,\n",
        "                        test_case_func_output,\n",
        "                        test_case_exp_output):\n",
        "  \n",
        "    print(f\"Input: {test_case_input}\")\n",
        "    print(f\"Function Output: {test_case_func_output}\")\n",
        "    print(f\"Expected Output: {test_case_exp_output}\")\n",
        "\n",
        "    if test_case_func_output == test_case_exp_output:\n",
        "        print(\"Test Case Passed :)\")\n",
        "        print(\"**********************************\\n\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Test Case Failed :(\")\n",
        "        print(\"**********************************\\n\")\n",
        "        return False\n",
        "\n",
        "print(\"Running Sample Test Cases\")\n",
        "print(\"Sample Test Case 1:\")\n",
        "test_case = \"Mr. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly normal!\"\n",
        "test_case_answer = \"mr and mrs dursley of number four privet drive were proud to say that they were perfectly normal\"\n",
        "test_case_student_answer = preprocess_pipeline(test_case)\n",
        "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
        "\n",
        "print(\"Sample Test Case 2:\")\n",
        "test_case = \"\\\"Little tyke,\\\" chortled Mr. Dursley as He left the house.\"\n",
        "test_case_answer = \"little tyke chortled mr dursley as he left the house\"\n",
        "test_case_student_answer = preprocess_pipeline(test_case)\n",
        "assert evaluate_string_test_cases(test_case, test_case_student_answer, test_case_answer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "593ca64f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6c2d3a86b88ef8761277b28b98aa30f1",
          "grade": false,
          "grade_id": "cell-9a5f9fae0f5eff1c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "593ca64f"
      },
      "outputs": [],
      "source": [
        "## Preprocess the dataset\n",
        "\n",
        "train_df[\"sentence\"] = train_df[\"sentence\"].apply(lambda x : preprocess_pipeline(x))\n",
        "test_df[\"sentence\"] = test_df[\"sentence\"].apply(lambda x : preprocess_pipeline(x))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6afbfd5",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "dd9963a72e2d8734bbda65faa6c3cc1c",
          "grade": false,
          "grade_id": "cell-1d9b7cdf2d10f0e4",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f6afbfd5"
      },
      "source": [
        "### Task 0.2: Create Vocabulary (0.25 Marks)\n",
        "\n",
        "Implement the `create_vocab` function below like you did last time. Do not forget using `nltk.tokenize.word_tokenize` to tokenize the text into words."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "43f01f82",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "2ec17f0aa2b148dee6e3549d46b84a8a",
          "grade": false,
          "grade_id": "cell-8893f5c7966540bb",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "43f01f82"
      },
      "outputs": [],
      "source": [
        "def create_vocab(documents):\n",
        "    \"\"\"\n",
        "    Given a list of documents each represented as a string,\n",
        "    create a word vocabulary containing all the words that occur\n",
        "    in these documents.\n",
        "    (0.25 Marks)\n",
        "\n",
        "    Inputs:\n",
        "        - documents (list) : A list with each element as a string representing a\n",
        "                            document.\n",
        "\n",
        "    Returns:\n",
        "        - vocab (list) : A **sorted** list containing all unique words in the\n",
        "                        documents\n",
        "\n",
        "    Example Input: ['john likes to watch movies mary likes movies too',\n",
        "                  'mary also likes to watch football games']\n",
        "\n",
        "    Expected Output: ['also',\n",
        "                    'football',\n",
        "                    'games',\n",
        "                    'john',\n",
        "                    'likes',\n",
        "                    'mary',\n",
        "                    'movies',\n",
        "                    'to',\n",
        "                    'too',\n",
        "                    'watch']\n",
        "\n",
        "\n",
        "    Hint: `nltk.tokenize.word_tokenize` function may come in handy\n",
        "\n",
        "    \"\"\"\n",
        "        \n",
        "    vocab = []\n",
        "\n",
        "    from nltk import word_tokenize\n",
        "    for document in tqdm(documents):\n",
        "      vocab += word_tokenize(document)\n",
        "    \n",
        "    vocab=list(set(vocab))\n",
        "\n",
        "    return sorted(vocab) # Don't change this"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6dea49b0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "18ae064ed1c0e93ed3991c01e48a09bc",
          "grade": true,
          "grade_id": "cell-e36c6b1e7f49a6af",
          "locked": true,
          "points": 0.25,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6dea49b0",
        "outputId": "429dc36d-5f64-4c78-a2ce-35c3947b933c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1:\n",
            "Input: ['john likes to watch movies mary likes movies too', 'mary also likes to watch football games']\n",
            "Function Output: ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n",
            "Expected Output: ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n",
            "Test Case Passed :)\n",
            "**********************************\n",
            "\n",
            "Sample Test Case 2:\n",
            "Input: ['We all live in a yellow submarine.', 'Yellow submarine, yellow submarine!!']\n",
            "Function Output: ['!', ',', '.', 'We', 'Yellow', 'a', 'all', 'in', 'live', 'submarine', 'yellow']\n",
            "Expected Output: ['!', ',', '.', 'We', 'Yellow', 'a', 'all', 'in', 'live', 'submarine', 'yellow']\n",
            "Test Case Passed :)\n",
            "**********************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def evaluate_list_test_cases(test_case_input,\n",
        "                        test_case_func_output,\n",
        "                        test_case_exp_output):\n",
        "  \n",
        "    print(f\"Input: {test_case_input}\")\n",
        "    print(f\"Function Output: {test_case_func_output}\")\n",
        "    print(f\"Expected Output: {test_case_exp_output}\")\n",
        "\n",
        "    if test_case_func_output == test_case_exp_output:\n",
        "        print(\"Test Case Passed :)\")\n",
        "        print(\"**********************************\\n\")\n",
        "        return True\n",
        "    else:\n",
        "        print(\"Test Case Failed :(\")\n",
        "        print(\"**********************************\\n\")\n",
        "        return False\n",
        "\n",
        "\n",
        "print(\"Running Sample Test Cases\")\n",
        "print(\"Sample Test Case 1:\")\n",
        "\n",
        "test_case = [\"john likes to watch movies mary likes movies too\",\n",
        "              \"mary also likes to watch football games\"]\n",
        "test_case_answer = ['also', 'football', 'games', 'john', 'likes', 'mary', 'movies', 'to', 'too', 'watch']\n",
        "test_case_student_answer = create_vocab(test_case)\n",
        "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
        "\n",
        "print(\"Sample Test Case 2:\")\n",
        "\n",
        "test_case = [\"We all live in a yellow submarine.\",\n",
        "             \"Yellow submarine, yellow submarine!!\"\n",
        "             ]\n",
        "test_case_answer = ['!', ',', '.', 'We', 'Yellow', 'a', 'all', 'in', 'live', 'submarine', 'yellow']\n",
        "test_case_student_answer = create_vocab(test_case)\n",
        "assert evaluate_list_test_cases(test_case, test_case_student_answer, test_case_answer)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c0067f2f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "df599261456ff2e80f0720aa52ebab42",
          "grade": false,
          "grade_id": "cell-c5e83435becaae76",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "c0067f2f"
      },
      "outputs": [],
      "source": [
        "# Create vocabulary from training data\n",
        "train_documents = train_df[\"sentence\"].values.tolist()\n",
        "train_vocab = create_vocab(train_documents)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a6d15dce",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "9650665f01daca43aeb0246257dc2690",
          "grade": false,
          "grade_id": "cell-448eebb0efc2ac9a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "a6d15dce"
      },
      "source": [
        "### Task 0.3: Get Word Frequencies (0.75 Marks)\n",
        "\n",
        "We define the normalized frequency of a word `w` in a corpus as:\n",
        "\n",
        "p(w) = Number of occurences of `w` in all documents / Total Number of occurences of all words in all documents\n",
        "\n",
        "Note that this is same as unigram probabilities discussed in Assignment2a as well as in the lectures.\n",
        "Word frequencies can be helpful as it can help us recognize the most common words which in most cases will be stop words as well as rare words that occur in the documents. Later we will be making use of word frequencies to create sentence representations, but for now just implement the `get_word_frequencies` below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a1a0db0d",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e761739dc7e28be9f9258cf325802642",
          "grade": false,
          "grade_id": "cell-3f09f88f24427b07",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "a1a0db0d"
      },
      "outputs": [],
      "source": [
        "def get_word_frequencies(documents):\n",
        "    \"\"\"\n",
        "    Gets the normalized frequency of each word w i.e. \n",
        "    p(w) =  #num_of_occurences_of_w / #total_occurences_of_all_words\n",
        "    present in documents\n",
        "    \n",
        "    Inputs:\n",
        "        - documents(list): A list of documents\n",
        "    \n",
        "    Returns:\n",
        "        - word2freq(dict): A dictionary containing words as keys\n",
        "                           and values as their corresponding frequencies\n",
        "    \n",
        "    \"\"\"\n",
        "    from nltk import word_tokenize\n",
        "    from tqdm import tqdm\n",
        "\n",
        "    word2freq = {}\n",
        "    count = 0\n",
        "    vocab=[]\n",
        "    all_words=[]\n",
        "    print('Preparing. . . . ')\n",
        "    for document in tqdm(documents):\n",
        "      tokens = word_tokenize(document)\n",
        "      count += len(tokens)\n",
        "      all_words+=tokens\n",
        "      vocab = list(set(vocab+tokens))\n",
        "    print('Finalizing. .. .')\n",
        "    for word in tqdm(vocab):\n",
        "      word2freq[word] = len(np.where(np.array(all_words)==word)[0])/count\n",
        "\n",
        "    return word2freq"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "da445e8e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "ccb64e4a880ecc730ef330b5b843ff04",
          "grade": true,
          "grade_id": "cell-bf6d404f736519d9",
          "locked": true,
          "points": 0.75,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "da445e8e",
        "outputId": "9d8cb94e-3ef7-40a9-de8f-39514614a6d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input Documents: ['john likes to watch movies mary likes movies too', 'mary also likes to watch football games']\n",
            "Output Word Frequencies: {'games': 0.0625, 'too': 0.0625, 'football': 0.0625, 'likes': 0.1875, 'john': 0.0625, 'watch': 0.125, 'also': 0.0625, 'mary': 0.125, 'movies': 0.125, 'to': 0.125}\n",
            "Expected Word Frequencies: {'john': 0.0625, 'likes': 0.1875, 'to': 0.125, 'watch': 0.125, 'movies': 0.125, 'mary': 0.125, 'too': 0.0625, 'also': 0.0625, 'football': 0.0625, 'games': 0.0625}\n",
            "****************************************\n",
            "\n",
            "Running Sample Test Case 2\n",
            "Input Documents: ['We all live in a yellow submarine.', 'Yellow submarine, yellow submarine!!']\n",
            "Output Word Frequencies: {',': 0.06666666666666667, 'We': 0.06666666666666667, 'all': 0.06666666666666667, 'a': 0.06666666666666667, 'Yellow': 0.06666666666666667, 'submarine': 0.2, 'yellow': 0.13333333333333333, 'live': 0.06666666666666667, 'in': 0.06666666666666667, '!': 0.13333333333333333, '.': 0.06666666666666667}\n",
            "Expected Word Frequencies: {'We': 0.06666666666666667, 'all': 0.06666666666666667, 'live': 0.06666666666666667, 'in': 0.06666666666666667, 'a': 0.06666666666666667, 'yellow': 0.13333333333333333, 'submarine': 0.2, '.': 0.06666666666666667, 'Yellow': 0.06666666666666667, ',': 0.06666666666666667, '!': 0.13333333333333333}\n",
            "****************************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def check_dicts_same(dict1, dict2):\n",
        "    if not isinstance(dict1, dict):\n",
        "        print(\"Your function output is not a dictionary!\")\n",
        "        return False\n",
        "    if len(dict1) != len(dict2):\n",
        "        return False\n",
        "    \n",
        "    for key in dict1:\n",
        "        val1 = dict1[key]\n",
        "        val2 = dict2[key]\n",
        "        if isinstance(val1, float) and isinstance(val1, float):\n",
        "            if not np.allclose(val1, val2, 1e-4):\n",
        "                return False\n",
        "        if val1 != val2:\n",
        "            return False\n",
        "    \n",
        "    return True\n",
        "    \n",
        "print(\"Running Sample Test Case 1\")\n",
        "sample_documents = [\n",
        "    'john likes to watch movies mary likes movies too',\n",
        "    'mary also likes to watch football games'\n",
        "]\n",
        "actual_word2freq = {'john': 0.0625,\n",
        "                     'likes': 0.1875,\n",
        "                     'to': 0.125,\n",
        "                     'watch': 0.125,\n",
        "                     'movies': 0.125,\n",
        "                     'mary': 0.125,\n",
        "                     'too': 0.0625,\n",
        "                     'also': 0.0625,\n",
        "                     'football': 0.0625,\n",
        "                     'games': 0.0625}\n",
        "\n",
        "output_word2freq = get_word_frequencies(sample_documents)\n",
        "print(f\"Input Documents: {sample_documents}\")\n",
        "print(f\"Output Word Frequencies: {output_word2freq}\")\n",
        "print(f\"Expected Word Frequencies: {actual_word2freq}\")\n",
        "\n",
        "assert check_dicts_same(output_word2freq, actual_word2freq)\n",
        "print(\"****************************************\\n\")\n",
        "\n",
        "print(\"Running Sample Test Case 2\")\n",
        "sample_documents = [\n",
        "    'We all live in a yellow submarine.',\n",
        "    'Yellow submarine, yellow submarine!!'\n",
        "]\n",
        "actual_word2freq = {'We': 0.06666666666666667,\n",
        "                    'all': 0.06666666666666667,\n",
        "                    'live': 0.06666666666666667,\n",
        "                    'in': 0.06666666666666667,\n",
        "                    'a': 0.06666666666666667,\n",
        "                    'yellow': 0.13333333333333333,\n",
        "                    'submarine': 0.2,\n",
        "                    '.': 0.06666666666666667,\n",
        "                    'Yellow': 0.06666666666666667,\n",
        "                    ',': 0.06666666666666667,\n",
        "                    '!': 0.13333333333333333}\n",
        "\n",
        "output_word2freq = get_word_frequencies(sample_documents)\n",
        "print(f\"Input Documents: {sample_documents}\")\n",
        "print(f\"Output Word Frequencies: {output_word2freq}\")\n",
        "print(f\"Expected Word Frequencies: {actual_word2freq}\")\n",
        "\n",
        "assert check_dicts_same(output_word2freq, actual_word2freq)\n",
        "print(\"****************************************\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8dbb793f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "035499b52a6e12a24adb1d6144527547",
          "grade": false,
          "grade_id": "cell-cb63290c9a9faade",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "8dbb793f"
      },
      "source": [
        "## Task 1: Word2Vec Representations\n",
        "\n",
        "In this task you will learn how to use word2vec for obtaining vector representations for words and then how to use them further to create sentence/document level vector representations. We will be using the popular [gensim](https://radimrehurek.com/gensim/) package that has great support for vector space models and supports various popular word embedding methods like word2vec, fasttext, LSA etc. For the purposes of this assignment we will be working with the pretrained word2vec vectors on the google news corpus containing about 100 billion tokens. Below we provide a tutorial on how to use gensim for obtaining these word vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4ee3942d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "a45f2072594bbd2ffe8a895b399958a3",
          "grade": false,
          "grade_id": "cell-9cbececf20649cac",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "4ee3942d"
      },
      "source": [
        "We start by downloading pretrained word2vec vectors and create a `gensim.models.keyedvectors` obect. The download has a size of about 2GB, so might take a few minutes to download and load. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e65ed16f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3a6ce1a75eb60a924ffd10d06a33d79d",
          "grade": false,
          "grade_id": "cell-73dd0118ce378a1a",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e65ed16f",
        "outputId": "6ee81aa5-65ce-49d7-e0c5-2125104b47b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[==================================================] 100.0% 1662.8/1662.8MB downloaded\n"
          ]
        }
      ],
      "source": [
        "import gensim.downloader as api\n",
        "wv = api.load('word2vec-google-news-300')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "298caa61",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6ba8ed299739fe897b0967aec789dfab",
          "grade": false,
          "grade_id": "cell-2ec190e6285691e1",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "298caa61"
      },
      "source": [
        "The `wv` object has a bunch of methods that we can use to obtain vector representations of words, finding similar words etc. We start with how to obtain vectors for words using it, which can be done using the `get_vector` method as demonstrated below."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# np.save('word2vec-google-news-300.npy',wv)\n",
        "\n",
        "wv=np.load('word2vec-google-news-300.npy')"
      ],
      "metadata": {
        "id": "pZEzPhlRb442"
      },
      "id": "pZEzPhlRb442",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "19cb7ee1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "abf33dde1d542b2b4fdcb72d32919d14",
          "grade": false,
          "grade_id": "cell-f6f0467707b76ef3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "19cb7ee1",
        "outputId": "0f9878df-c68a-4060-f3ef-9b000786dd0c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word : bad\n",
            "Length of the vector: 300\n",
            "Vector:\n",
            "[ 0.06298828  0.12451172  0.11328125  0.07324219  0.03881836  0.07910156\n",
            "  0.05078125  0.171875    0.09619141  0.22070312 -0.04150391 -0.09277344\n",
            " -0.02209473  0.14746094 -0.21582031  0.15234375  0.19238281 -0.05078125\n",
            " -0.11181641 -0.3203125   0.00506592  0.15332031 -0.02563477 -0.0234375\n",
            "  0.36328125  0.20605469  0.04760742 -0.02624512  0.09033203  0.00457764\n",
            " -0.15332031  0.06591797  0.3515625  -0.12451172  0.03015137  0.16210938\n",
            "  0.00242615 -0.02282715  0.02978516  0.00531006  0.25976562 -0.22460938\n",
            "  0.29492188 -0.18066406  0.07910156  0.02282715  0.12109375 -0.17382812\n",
            " -0.03735352 -0.06933594 -0.21972656  0.1875     -0.03320312 -0.06225586\n",
            " -0.04492188  0.11621094 -0.23339844 -0.11669922  0.09814453 -0.11962891\n",
            "  0.13964844  0.28710938 -0.26953125 -0.05493164  0.03112793 -0.05029297\n",
            "  0.1328125  -0.01831055 -0.37695312 -0.06298828  0.12597656 -0.07910156\n",
            " -0.04467773  0.10400391 -0.41210938  0.22851562 -0.07080078  0.24511719\n",
            "  0.06494141  0.12890625 -0.05102539 -0.00308228 -0.17871094  0.25976562\n",
            " -0.13476562 -0.21289062 -0.234375    0.21777344 -0.07910156  0.01977539\n",
            "  0.19726562  0.17285156  0.03613281 -0.17578125 -0.02966309 -0.00939941\n",
            "  0.25976562  0.12353516  0.19140625 -0.03930664  0.15917969  0.05664062\n",
            " -0.01977539 -0.14941406  0.12597656 -0.00350952 -0.05957031 -0.14648438\n",
            "  0.01660156 -0.35742188 -0.0300293   0.03149414 -0.0324707  -0.3203125\n",
            "  0.35351562 -0.19433594  0.13964844  0.07470703 -0.10888672  0.10107422\n",
            " -0.296875   -0.01348877 -0.14160156  0.06982422 -0.20703125 -0.25195312\n",
            "  0.03955078  0.04345703  0.05957031 -0.15429688 -0.43359375 -0.13671875\n",
            "  0.00436401  0.13867188 -0.13867188 -0.125       0.00118256  0.08203125\n",
            " -0.01989746 -0.10449219  0.04638672  0.03735352  0.078125   -0.00656128\n",
            " -0.12402344 -0.3125     -0.23046875  0.0065918   0.22949219 -0.21875\n",
            "  0.2421875  -0.01062012 -0.26367188  0.3359375  -0.19140625  0.02636719\n",
            " -0.0112915  -0.20898438  0.06298828 -0.07763672 -0.11572266  0.14648438\n",
            "  0.10400391 -0.02819824  0.12109375 -0.11083984 -0.02893066 -0.171875\n",
            "  0.1953125  -0.12451172 -0.19140625 -0.03857422 -0.01507568  0.05151367\n",
            " -0.06884766  0.07177734  0.25195312 -0.09570312  0.08251953 -0.0135498\n",
            "  0.07177734 -0.27734375  0.00350952 -0.11035156 -0.15039062  0.08642578\n",
            " -0.27148438  0.10009766 -0.02746582  0.07470703  0.11865234  0.08740234\n",
            " -0.03955078  0.05004883 -0.03735352  0.03369141 -0.01977539 -0.16210938\n",
            "  0.00460815 -0.0390625   0.10302734  0.18066406 -0.01495361 -0.08105469\n",
            "  0.02905273 -0.02490234 -0.21875     0.04492188 -0.09472656 -0.07519531\n",
            " -0.1640625  -0.13476562  0.02111816  0.10888672 -0.08251953  0.10644531\n",
            "  0.04345703 -0.1484375  -0.02038574  0.02734375 -0.11767578 -0.03735352\n",
            "  0.10400391 -0.11572266  0.0546875  -0.05664062 -0.11669922  0.00180817\n",
            " -0.04736328  0.13085938 -0.00089645  0.01831055  0.13378906 -0.12060547\n",
            "  0.13671875  0.05053711 -0.19238281 -0.24414062  0.02062988  0.11035156\n",
            "  0.42773438  0.11572266  0.0480957  -0.11572266  0.00787354 -0.08251953\n",
            "  0.03808594  0.06542969 -0.14453125 -0.13769531  0.02001953 -0.05395508\n",
            "  0.17675781  0.06298828 -0.05981445 -0.25195312  0.24414062  0.17382812\n",
            "  0.09619141 -0.30664062 -0.21875     0.28710938 -0.00897217  0.01818848\n",
            "  0.06445312  0.01660156 -0.07177734 -0.15625     0.06738281 -0.05371094\n",
            "  0.08154297  0.29101562  0.11523438 -0.02258301  0.01306152 -0.10595703\n",
            "  0.19824219 -0.03393555 -0.05419922  0.07763672  0.05859375 -0.07910156\n",
            "  0.09863281 -0.06054688 -0.09765625 -0.01269531 -0.12695312 -0.06982422\n",
            " -0.13574219 -0.10058594  0.01135254  0.34179688 -0.09033203  0.07666016\n",
            " -0.0324707   0.13378906 -0.15429688 -0.06347656  0.11474609  0.03100586]\n"
          ]
        }
      ],
      "source": [
        "word = \"bad\"\n",
        "vector = wv.get_vector(word)\n",
        "print(f\"Word : {word}\")\n",
        "print(f\"Length of the vector: {len(vector)}\")\n",
        "print(f\"Vector:\")\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1f3f78a9",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0ebdab812a9479411729817e75a1946c",
          "grade": false,
          "grade_id": "cell-e8ea71f63cd29438",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1f3f78a9"
      },
      "source": [
        "You can also obtain the brackets by using angular brackets notation i.e. `wv[\"bad\"]`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3f17f380",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3f17f380",
        "outputId": "7088208d-57bf-4565-bce2-97346330eb64"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word : bad\n",
            "Length of the vector: 300\n",
            "Vector:\n",
            "[ 0.06298828  0.12451172  0.11328125  0.07324219  0.03881836  0.07910156\n",
            "  0.05078125  0.171875    0.09619141  0.22070312 -0.04150391 -0.09277344\n",
            " -0.02209473  0.14746094 -0.21582031  0.15234375  0.19238281 -0.05078125\n",
            " -0.11181641 -0.3203125   0.00506592  0.15332031 -0.02563477 -0.0234375\n",
            "  0.36328125  0.20605469  0.04760742 -0.02624512  0.09033203  0.00457764\n",
            " -0.15332031  0.06591797  0.3515625  -0.12451172  0.03015137  0.16210938\n",
            "  0.00242615 -0.02282715  0.02978516  0.00531006  0.25976562 -0.22460938\n",
            "  0.29492188 -0.18066406  0.07910156  0.02282715  0.12109375 -0.17382812\n",
            " -0.03735352 -0.06933594 -0.21972656  0.1875     -0.03320312 -0.06225586\n",
            " -0.04492188  0.11621094 -0.23339844 -0.11669922  0.09814453 -0.11962891\n",
            "  0.13964844  0.28710938 -0.26953125 -0.05493164  0.03112793 -0.05029297\n",
            "  0.1328125  -0.01831055 -0.37695312 -0.06298828  0.12597656 -0.07910156\n",
            " -0.04467773  0.10400391 -0.41210938  0.22851562 -0.07080078  0.24511719\n",
            "  0.06494141  0.12890625 -0.05102539 -0.00308228 -0.17871094  0.25976562\n",
            " -0.13476562 -0.21289062 -0.234375    0.21777344 -0.07910156  0.01977539\n",
            "  0.19726562  0.17285156  0.03613281 -0.17578125 -0.02966309 -0.00939941\n",
            "  0.25976562  0.12353516  0.19140625 -0.03930664  0.15917969  0.05664062\n",
            " -0.01977539 -0.14941406  0.12597656 -0.00350952 -0.05957031 -0.14648438\n",
            "  0.01660156 -0.35742188 -0.0300293   0.03149414 -0.0324707  -0.3203125\n",
            "  0.35351562 -0.19433594  0.13964844  0.07470703 -0.10888672  0.10107422\n",
            " -0.296875   -0.01348877 -0.14160156  0.06982422 -0.20703125 -0.25195312\n",
            "  0.03955078  0.04345703  0.05957031 -0.15429688 -0.43359375 -0.13671875\n",
            "  0.00436401  0.13867188 -0.13867188 -0.125       0.00118256  0.08203125\n",
            " -0.01989746 -0.10449219  0.04638672  0.03735352  0.078125   -0.00656128\n",
            " -0.12402344 -0.3125     -0.23046875  0.0065918   0.22949219 -0.21875\n",
            "  0.2421875  -0.01062012 -0.26367188  0.3359375  -0.19140625  0.02636719\n",
            " -0.0112915  -0.20898438  0.06298828 -0.07763672 -0.11572266  0.14648438\n",
            "  0.10400391 -0.02819824  0.12109375 -0.11083984 -0.02893066 -0.171875\n",
            "  0.1953125  -0.12451172 -0.19140625 -0.03857422 -0.01507568  0.05151367\n",
            " -0.06884766  0.07177734  0.25195312 -0.09570312  0.08251953 -0.0135498\n",
            "  0.07177734 -0.27734375  0.00350952 -0.11035156 -0.15039062  0.08642578\n",
            " -0.27148438  0.10009766 -0.02746582  0.07470703  0.11865234  0.08740234\n",
            " -0.03955078  0.05004883 -0.03735352  0.03369141 -0.01977539 -0.16210938\n",
            "  0.00460815 -0.0390625   0.10302734  0.18066406 -0.01495361 -0.08105469\n",
            "  0.02905273 -0.02490234 -0.21875     0.04492188 -0.09472656 -0.07519531\n",
            " -0.1640625  -0.13476562  0.02111816  0.10888672 -0.08251953  0.10644531\n",
            "  0.04345703 -0.1484375  -0.02038574  0.02734375 -0.11767578 -0.03735352\n",
            "  0.10400391 -0.11572266  0.0546875  -0.05664062 -0.11669922  0.00180817\n",
            " -0.04736328  0.13085938 -0.00089645  0.01831055  0.13378906 -0.12060547\n",
            "  0.13671875  0.05053711 -0.19238281 -0.24414062  0.02062988  0.11035156\n",
            "  0.42773438  0.11572266  0.0480957  -0.11572266  0.00787354 -0.08251953\n",
            "  0.03808594  0.06542969 -0.14453125 -0.13769531  0.02001953 -0.05395508\n",
            "  0.17675781  0.06298828 -0.05981445 -0.25195312  0.24414062  0.17382812\n",
            "  0.09619141 -0.30664062 -0.21875     0.28710938 -0.00897217  0.01818848\n",
            "  0.06445312  0.01660156 -0.07177734 -0.15625     0.06738281 -0.05371094\n",
            "  0.08154297  0.29101562  0.11523438 -0.02258301  0.01306152 -0.10595703\n",
            "  0.19824219 -0.03393555 -0.05419922  0.07763672  0.05859375 -0.07910156\n",
            "  0.09863281 -0.06054688 -0.09765625 -0.01269531 -0.12695312 -0.06982422\n",
            " -0.13574219 -0.10058594  0.01135254  0.34179688 -0.09033203  0.07666016\n",
            " -0.0324707   0.13378906 -0.15429688 -0.06347656  0.11474609  0.03100586]\n"
          ]
        }
      ],
      "source": [
        "word = \"bad\"\n",
        "vector = wv[word]\n",
        "print(f\"Word : {word}\")\n",
        "print(f\"Length of the vector: {len(vector)}\")\n",
        "print(f\"Vector:\")\n",
        "print(vector)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b2ea9cdd",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "879b6981a00a275555d915d274403d40",
          "grade": false,
          "grade_id": "cell-8e044d48bc3d945c",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "b2ea9cdd"
      },
      "source": [
        "Also note that the word2vec model might not have vectors for all words, you can check for Out of Vocabulary (OOV) words using the `in` operator as shown in the code block below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e1f75bce",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "960f2b8166d838e07b5f0ba258db7c1c",
          "grade": false,
          "grade_id": "cell-66a1c4a28ac5655d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e1f75bce",
        "outputId": "cb7efa29-f225-4871-8ebc-248cef94677a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "True\n",
            "False\n"
          ]
        }
      ],
      "source": [
        "print(\"book\" in wv)\n",
        "print(\"blastoise\" in wv)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "278bd4d0",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d2454333a3d0950ecc2ad906062ce294",
          "grade": false,
          "grade_id": "cell-af317103566b235b",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "278bd4d0"
      },
      "source": [
        "Just looking at the vectors we cannot really gain any insights about them, but it is the relation between the vectors of different words that is much more easier to interpet. `wv` object has a `most_similar` method that for a given word obtains the words that are most similar to it by computing cosine similarity between them."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Running the below 4 cells were crashing the "
      ],
      "metadata": {
        "id": "Rfe0ECODvfQy"
      },
      "id": "Rfe0ECODvfQy",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "42f0669e",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "3354f1ff87379df70f29be49745cfc5d",
          "grade": false,
          "grade_id": "cell-ac4f122af54887f6",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "42f0669e"
      },
      "outputs": [],
      "source": [
        "wv.most_similar(\"bad\",topn=5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dad7571",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "edb14356c0c0783c5f39c0f2898156d1",
          "grade": false,
          "grade_id": "cell-6aac9038cfba8b93",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "7dad7571"
      },
      "outputs": [],
      "source": [
        "wv.most_similar(\"king\",topn=5)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f9319424",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "04e84bcad2385b88001486831b87bbec",
          "grade": false,
          "grade_id": "cell-c4c59eb5b19eb50f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "f9319424"
      },
      "source": [
        "You can see that the we obtain very reasonable similar words in both examples. We can also use `most_similar` to do the analogy comparison that was discussed in the class. For eg: man : king :: woman : ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0499db84",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "553f0b0d30babf6f8e31370f26a92416",
          "grade": false,
          "grade_id": "cell-7a44322336c7d397",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0499db84"
      },
      "outputs": [],
      "source": [
        "wv.most_similar(positive=['woman', 'king'], negative=['man'], topn = 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "624f1241",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "c82ad7139468ace874097a6b68fc20ca",
          "grade": false,
          "grade_id": "cell-c1c43a088c7b5158",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "624f1241"
      },
      "outputs": [],
      "source": [
        "wv.most_similar(positive=['woman', 'father'], negative=['man'], topn = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "df93c9b8",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "310c1f00d90c5fd10b824d1d81918181",
          "grade": false,
          "grade_id": "cell-a875955d15d39023",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "df93c9b8"
      },
      "source": [
        "### Task 1.1 Sentence representations using Word2Vec : Bag of Words Methods (1 Mark)\n",
        "\n",
        "Now that we know how to obtain the vectors of each word, how can we obtain a vector representation for a sentence or a document? One of the simplest way is to add the vectors of all the words in the sentence to obtain sentence vector. This is also called the Bag of Words approach. Can you think of why? Last time when we discussed bag of words features for a sentence, it contained counts of each word occuring in the sentence. This can be just thought of as just adding one hot vectors for all the words in a sentence. Hence, adding word2vec vectors for each word in the sentence can also be viewed as a bag of words representation.\n",
        "\n",
        "Implement the `get_bow_sent_vec` function below that takes in a sentence and adds the word2vec vectors for each word occuring in the sentence to obtain the sentence vector. Also, in practice it is helpful to divide the sum of word vectors by the number of words to normalize the representation obtained."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7d348d35",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "a35f65d1938b1db128b3a55a8c6f1c1e",
          "grade": false,
          "grade_id": "cell-c0d2059784aa66e3",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "7d348d35"
      },
      "outputs": [],
      "source": [
        "def get_bow_sent_vec(sentence, wv):\n",
        "    \"\"\"\n",
        "    Obtains the vector representation of a sentence by adding the word vectors\n",
        "    for each word occuring in the sentence (and dividing by the number of words) i.e\n",
        "    \n",
        "    v(s) = sum_{w \\in s}(v(w)) / N(s)\n",
        "    where N(s) is the number of words in the sentence,\n",
        "    v(w) is the word2vec representation for word w\n",
        "    and v(s) is the obtained vector representation of sentence s\n",
        "    \n",
        "    Inputs:\n",
        "        - sentence (str): A string containing the sentence to be encoded\n",
        "        - wv (gensim.models.keyedvectors.KeyedVectors) : A gensim word vector model object.\n",
        "        \n",
        "    Returns:\n",
        "        - sentence_vec (np.ndarray): A numpy array containing the vector representation\n",
        "        of the sentence\n",
        "        \n",
        "    Note : Not all the words might be present in `wv` so you will need to check for that,\n",
        "          and only add vectors for the words that are present. Also while normalization\n",
        "          divide by the number of words for which a word vector was actually present in `wv`\n",
        "    \n",
        "    Important Note: In case no word in the sentence is present in `wv`, return an all zero vector!\n",
        "\n",
        "    \"\"\"\n",
        "    from nltk import word_tokenize\n",
        "    sentence_vec = np.zeros(300)\n",
        "    \n",
        "    tokens = word_tokenize(sentence)\n",
        "    adjuster=0\n",
        "    for word in tokens:\n",
        "      try:\n",
        "        sentence_vec += wv[word]\n",
        "      except:\n",
        "        adjuster +=1\n",
        "    # Normalising\n",
        "    if (len(tokens)-adjuster)>0:\n",
        "      sentence_vec /= (len(tokens)-adjuster)\n",
        "    else:\n",
        "      pass\n",
        "\n",
        "    return sentence_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1fd545f",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "42eb4a0f9c65990a3370e9cec278cb7c",
          "grade": true,
          "grade_id": "cell-89f0581a4dd4e139",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f1fd545f",
        "outputId": "054c584c-3756-455f-b072-af1e68950641"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input Sentence: john likes watching movies mary likes movies too\n",
            "First five elements of output vector: [ 0.03330994  0.11713409  0.00738525  0.24951172 -0.0202179 ]\n",
            "Expected first five elements of output vector: [ 0.03330994  0.11713409  0.00738525  0.24951172 -0.0202179 ]\n",
            "Sample Test Case Passed\n",
            "*******************************\n",
            "\n",
            "Running Sample Test Case 2\n",
            "Input Sentence: We all live in a yellow submarine.\n",
            "First five elements of output vector: [-0.08424886  0.14601644  0.0727946   0.09978231 -0.02655029]\n",
            "Expected first five elements of output vector: [-0.08424886  0.14601644  0.0727946   0.09978231 -0.02655029]\n",
            "Sample Test Case Passed\n",
            "*******************************\n",
            "\n",
            "Running Sample Test Case 3\n",
            "Input Sentence: blastoise pikachu charizard\n",
            "First five elements of output vector: [0. 0. 0. 0. 0.]\n",
            "Expected first five elements of output vector: [0. 0. 0. 0. 0.]\n",
            "Sample Test Case Passed\n",
            "*******************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Case 1\")\n",
        "sample_sentence ='john likes watching movies mary likes movies too'\n",
        "sentence_vec = get_bow_sent_vec(sample_sentence, wv)\n",
        "expected_sent_vec = np.array([ 0.03330994,  0.11713409,  0.00738525,  0.24951172, -0.0202179 ])\n",
        "print(f\"Input Sentence: {sample_sentence}\")\n",
        "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
        "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
        "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4) \n",
        "print(\"Sample Test Case Passed\")\n",
        "print(\"*******************************\\n\")\n",
        "\n",
        "print(\"Running Sample Test Case 2\")\n",
        "sample_sentence ='We all live in a yellow submarine.'\n",
        "sentence_vec = get_bow_sent_vec(sample_sentence, wv)\n",
        "expected_sent_vec = np.array([-0.08424886,  0.14601644,  0.0727946 ,  0.09978231, -0.02655029])\n",
        "print(f\"Input Sentence: {sample_sentence}\")\n",
        "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
        "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
        "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4) \n",
        "print(\"Sample Test Case Passed\")\n",
        "print(\"*******************************\\n\")\n",
        "\n",
        "print(\"Running Sample Test Case 3\")\n",
        "sample_sentence ='blastoise pikachu charizard'\n",
        "sentence_vec = get_bow_sent_vec(sample_sentence, wv)\n",
        "expected_sent_vec = np.array([0.,  0.,  0. ,  0., 0.])\n",
        "print(f\"Input Sentence: {sample_sentence}\")\n",
        "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
        "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
        "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4) \n",
        "print(\"Sample Test Case Passed\")\n",
        "print(\"*******************************\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2e10d6cf",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "cc0737ac96eebfe7b66e56800a6d3330",
          "grade": false,
          "grade_id": "cell-0fc251fe019587b7",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "2e10d6cf"
      },
      "source": [
        "### Task 1.2 Sentence representations using Word2Vec : Inverse Frequency Weighted Sum Method (1 Mark)\n",
        "\n",
        "Instead of directly adding the vectors for all the words in the sentence, we can do something slightly better which tends to work very well in practice. [Arora et al. 2017](https://openreview.net/pdf?id=SyK00v5xx) proposes the following method for computing sentence embedding from word vectors\n",
        "\n",
        "<img src=\"https://i.ibb.co/vwzHXHy/sent-embed.jpg\" alt=\"sent-embed\" border=\"0\">\n",
        "\n",
        "Here v_w is the vector representation of the word w, p(w) is the frequency of the word w, |s| is the number of words in the sentence, and `a` is just a constant with a typical value between 1e-3 to 1e-4.\n",
        "\n",
        "Intuitively, we take a weighted sum of all the word vectors where the weights are inversely propotional to the frequency of the word (p(w)). This ensures that very frequent words which are often stop words like \"the\", \"I\" etc. are given lower weightage when constructing the sentence vector. `a` is used as smoothing constant, such that when p(w) = 0 we still have finite weights.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "63553403",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7ed68271d7af177dd6531cb24506ce52",
          "grade": false,
          "grade_id": "cell-19256c98f438046c",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "63553403"
      },
      "outputs": [],
      "source": [
        "def get_weighted_bow_sent_vec(sentence, wv, word2freq, a = 1e-4):\n",
        "    \"\"\"\n",
        "    Obtains the vector representation of a sentence by adding the word vectors\n",
        "    for each word occuring in the sentence (and dividing by the number of words) i.e\n",
        "    \n",
        "    v(s) = (sum_{w \\in s} a / (a + p(w)) * (v(w))) / N(s)\n",
        "    \n",
        "    Inputs:\n",
        "        - sentence (str): A string containing the sentence to be encoded\n",
        "        - wv (gensim.models.keyedvectors.KeyedVectors) : A gensim word vector model object.\n",
        "        - word2freq (dict): A dictionary with words as keys and their frequency in the\n",
        "                            entire training dataset as values\n",
        "        - a (float): Smoothing constant\n",
        "        \n",
        "    Returns:\n",
        "        - sentence_vec (np.ndarray): A numpy array containing the vector representation\n",
        "        of the sentence\n",
        "    \n",
        "    Important Note: In case no word in the sentence is present in `wv`, return an all zero vector!\n",
        "    \n",
        "    Hint: If a word is not present in the `word2freq` dictionary, you can consider frequency\n",
        "          of that word to be zero\n",
        "        \n",
        "    \"\"\"\n",
        "    \n",
        "    sentence_vec = np.zeros(300)\n",
        "    \n",
        "    from nltk import word_tokenize\n",
        "    tokens = word_tokenize( sentence )\n",
        "    s = len(tokens)\n",
        "    reduce_count = 0\n",
        "    for word in tokens : \n",
        "      try:\n",
        "        wv_value=wv[word]\n",
        "        try:\n",
        "          sentence_vec += (wv[word] * (a/(a + word2freq[word])))\n",
        "        except:\n",
        "          sentence_vec += (wv[word] * (a/(a + 0)))\n",
        "      except:\n",
        "        reduce_count +=1\n",
        "\n",
        "    if s-reduce_count >0:\n",
        "      sentence_vec /= (s-reduce_count)\n",
        "    else:\n",
        "      pass\n",
        "    return sentence_vec"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6457e35c",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "cfecfe6152307e9d8feef730e6a70d7e",
          "grade": true,
          "grade_id": "cell-eca82e8303f85b83",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6457e35c",
        "outputId": "529efa75-6ad1-4ebc-89aa-c0ad896e10a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Case 1\n",
            "Input Sentence: john likes watching movies mary likes movies too\n",
            "First five elements of output vector: [-0.00384654  0.00208942  0.00010824  0.00648482 -0.00236967]\n",
            "Expected first five elements of output vector: [-0.00384654  0.00208942  0.00010824  0.00648482 -0.00236967]\n",
            "Sample Test Case Passed\n",
            "*******************************\n",
            "\n",
            "Running Sample Test Case 2\n",
            "Input Sentence: We all live in a yellow submarine.\n",
            "First five elements of output vector: [-0.08424886  0.14601644  0.0727946   0.09978231 -0.02655029]\n",
            "Expected first five elements of output vector: [-0.08424886  0.14601644  0.0727946   0.09978231 -0.02655029]\n",
            "Sample Test Case Passed\n",
            "*******************************\n",
            "\n",
            "Running Sample Test Case 3\n",
            "Input Sentence: blastoise pikachu charizard\n",
            "First five elements of output vector: [0. 0. 0. 0. 0.]\n",
            "Expected first five elements of output vector: [0. 0. 0. 0. 0.]\n",
            "Sample Test Case Passed\n",
            "*******************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Case 1\")\n",
        "sample_sentence ='john likes watching movies mary likes movies too'\n",
        "sample_word2freq = {\n",
        "    \"john\" : 0.001,\n",
        "    \"likes\": 0.01,\n",
        "    \"watching\" : 0.01,\n",
        "    \"movies\": 0.05,\n",
        "    \"mary\" : 0.001,\n",
        "    \"too\": 0.1\n",
        "}\n",
        "sentence_vec = get_weighted_bow_sent_vec(sample_sentence, wv, sample_word2freq)\n",
        "expected_sent_vec = np.array([-0.00384654,  0.00208942,  0.00010824,  0.00648482, -0.00236967])\n",
        "print(f\"Input Sentence: {sample_sentence}\")\n",
        "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
        "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
        "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4) \n",
        "print(\"Sample Test Case Passed\")\n",
        "print(\"*******************************\\n\")\n",
        "\n",
        "print(\"Running Sample Test Case 2\")\n",
        "sample_sentence ='We all live in a yellow submarine.'\n",
        "sentence_vec = get_weighted_bow_sent_vec(sample_sentence, wv, word2freq = {}, a = 1e-3)\n",
        "expected_sent_vec = np.array([-0.08424886,  0.14601644,  0.0727946 ,  0.09978231, -0.02655029])\n",
        "print(f\"Input Sentence: {sample_sentence}\")\n",
        "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
        "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
        "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4) \n",
        "print(\"Sample Test Case Passed\")\n",
        "print(\"*******************************\\n\")\n",
        "\n",
        "print(\"Running Sample Test Case 3\")\n",
        "sample_sentence ='blastoise pikachu charizard'\n",
        "sentence_vec = get_weighted_bow_sent_vec(sample_sentence, wv, word2freq = {}, a = 1e-3)\n",
        "expected_sent_vec = np.array([0.,  0.,  0. ,  0., 0.])\n",
        "print(f\"Input Sentence: {sample_sentence}\")\n",
        "print(f\"First five elements of output vector: {sentence_vec[:5]}\")\n",
        "print(f\"Expected first five elements of output vector: {expected_sent_vec}\")\n",
        "assert np.allclose(sentence_vec[:5], expected_sent_vec, 1e-4) \n",
        "print(\"Sample Test Case Passed\")\n",
        "print(\"*******************************\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "55e02b99",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "bbc1d5777439da6a7b4e7e6d82925532",
          "grade": false,
          "grade_id": "cell-291c9ad1bba3a8e3",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "55e02b99"
      },
      "source": [
        "Now that you have implemented the sentence vector functions, let's obtain sentence vectors for all the sentences in our training and test sets. This will take a few minutes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6f7f493b",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e84c62f929096253f4fbe19e8f00af81",
          "grade": false,
          "grade_id": "cell-59a4971906e0da17",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f7f493b",
        "outputId": "064542ec-26d7-46a3-ee2f-4ed2ef19e840"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Preparing. . . . \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 67349/67349 [03:31<00:00, 318.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finalizing. .. .\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 14704/14704 [30:53<00:00,  7.93it/s]\n"
          ]
        }
      ],
      "source": [
        "train_documents = train_df[\"sentence\"].values.tolist()\n",
        "test_documents = test_df[\"sentence\"].values.tolist()\n",
        "train_vocab = create_vocab(train_documents)\n",
        "train_word2freq = get_word_frequencies(train_documents)\n",
        "\n",
        "train_bow_vectors = np.array([\n",
        "    get_bow_sent_vec(document, wv)\n",
        "    for document in train_documents\n",
        "])\n",
        "test_bow_vectors = np.array([\n",
        "    get_bow_sent_vec(document, wv)\n",
        "    for document in test_documents\n",
        "])\n",
        "\n",
        "train_w_bow_vectors = np.array([\n",
        "    get_weighted_bow_sent_vec(document, wv, train_word2freq, a = 1e-3)\n",
        "    for document in train_documents\n",
        "])\n",
        "test_w_bow_vectors = np.array([\n",
        "    get_weighted_bow_sent_vec(document, wv, train_word2freq, a = 1e-3)\n",
        "    for document in test_documents\n",
        "])"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# # saving the data to reduce execution time on next run\n",
        "# np.save('train_documents.npy',train_documents)\n",
        "# np.save('test_documents.npy',test_documents)\n",
        "# np.save('train_vocab .npy',train_vocab )\n",
        "# np.save('train_word2freq.npy',train_word2freq)"
      ],
      "metadata": {
        "id": "TAeUHAMwt9X9"
      },
      "id": "TAeUHAMwt9X9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_documents = np.load('train_documents.npy')\n",
        "test_documents = np.load('test_documents.npy')\n",
        "train_vocab = np.load('train_vocab.npy')\n",
        "train_word2freq = np.load('train_word2freq.npy')\n"
      ],
      "metadata": {
        "id": "btv4UXCGuXuk"
      },
      "id": "btv4UXCGuXuk",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "15d964a1",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "c21eab906b95f945b802ebcb80135ac6",
          "grade": false,
          "grade_id": "cell-87c866ac94ce9071",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "15d964a1"
      },
      "source": [
        "## Task 2: Train a Sentiment Classifier using Sentence Vectors\n",
        "\n",
        "This part will be just like Assignment 1, but instead of the Bag of Word features we defined last time to train the classifier, we will use the sentence vectors obtained from word2vec."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "41dab076",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "0e796dcedbd4c53fd32fbff12a6baa59",
          "grade": false,
          "grade_id": "cell-ef5aff9d05dd7f03",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "41dab076"
      },
      "source": [
        "### Define a Custom Dataset class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c5530c3d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "f868905b82db877c8d42a9e1c5f8a867",
          "grade": false,
          "grade_id": "cell-7d07e1f5c96cba40",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "c5530c3d"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SST2Dataset(Dataset):\n",
        "    \n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        \n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "    \n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]\n",
        "    \n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1340d830",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "4fa0079f22fd50ec90d7ac1a17050c25",
          "grade": false,
          "grade_id": "cell-31d60a7015686bb5",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "1340d830"
      },
      "source": [
        "### Task 2.1: Define the Logistic Regression Model (1 Mark)\n",
        "\n",
        "Like last time define a Logistic Regression model that takes as input the sentence vector and predicts the label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cff5bcc2",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "7a2d6354df4d92cc565483ae09825f4d",
          "grade": false,
          "grade_id": "cell-42cde4e860bc4f41",
          "locked": false,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "cff5bcc2"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class LogisticRegressionModel(nn.Module):\n",
        "    \n",
        "    def __init__(self, d_input):\n",
        "        \"\"\"\n",
        "        Define the architecture of a Logistic Regression classifier.\n",
        "        You will need to define two components, one will be the linear layer using\n",
        "        nn.Linear, and a sigmoid activation function for the output.\n",
        "\n",
        "        Inputs:\n",
        "          - d_input (int): The dimensionality or number of features in each input. \n",
        "                            This will be required to define the linear layer\n",
        "\n",
        "        Hint: Recall that in logistic regression we obtain a single probablility\n",
        "        value for each input that denotes how likely is the input belonging\n",
        "        to the positive class\n",
        "        \"\"\"\n",
        "        #Need to call the constructor of the parent class\n",
        "        super(LogisticRegressionModel, self).__init__()\n",
        "\n",
        "        self.linear_layer = nn.Linear ( d_input ,1 )\n",
        "        self.sigmoid_layer = nn.Sigmoid()\n",
        "\n",
        "\n",
        "  \n",
        "    def forward(self, x):\n",
        "        \"\"\"\n",
        "        Passes the input `x` through the layers in the network and returns the output\n",
        "\n",
        "        Inputs:\n",
        "          - x (torch.tensor): A torch tensor of shape [batch_size, d_input] representing the batch of inputs\n",
        "\n",
        "        Returns:\n",
        "          - output (torch.tensor): A torch tensor of shape [batch_size,] obtained after passing the input to the network\n",
        "\n",
        "        \"\"\"\n",
        "        output = self.linear_layer(x)\n",
        "        output = self.sigmoid_layer(output)\n",
        "        \n",
        "\n",
        "        return output.squeeze(-1) # Question: Why do squeeze() here? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e33d5c8d",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "20900a835cc720b8a9917d2f1c485802",
          "grade": true,
          "grade_id": "cell-3abd5485d61989dd",
          "locked": true,
          "points": 1,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e33d5c8d",
        "outputId": "eb4936a2-01a7-4439-edc8-56f2df93f96d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Sample Test Cases\n",
            "Sample Test Case 1: Testing linear layer input and output sizes, for d_input = 5\n",
            "Number of Input Features: 5\n",
            "Number of Output Features: 1\n",
            "Expected Number of Input Features: 5\n",
            "Expected Number of Output Features: 1\n",
            "**********************************\n",
            "\n",
            "Sample Test Case 2: Testing linear layer input and output sizes, for d_input = 24\n",
            "Number of Input Features: 24\n",
            "Number of Output Features: 1\n",
            "Expected Number of Input Features: 24\n",
            "Expected Number of Output Features: 1\n",
            "**********************************\n",
            "\n",
            "Sample Test Case 3: Checking if the model gives correct output\n",
            "Model Output: 0.6298196315765381\n",
            "Expected Output: 0.6298196315765381\n",
            "**********************************\n",
            "\n",
            "Sample Test Case 4: Checking if the model gives correct output\n",
            "Model Output: [0.5503339 0.5428218 0.561816  0.51846  ]\n",
            "Expected Output: [0.5503339 0.5428218 0.561816  0.51846  ]\n",
            "**********************************\n",
            "\n"
          ]
        }
      ],
      "source": [
        "print(\"Running Sample Test Cases\")\n",
        "torch.manual_seed(42)\n",
        "d_input = 5\n",
        "sample_lr_model = LogisticRegressionModel(d_input = d_input)\n",
        "print(f\"Sample Test Case 1: Testing linear layer input and output sizes, for d_input = {d_input}\")\n",
        "in_features = sample_lr_model.linear_layer.in_features\n",
        "out_features = sample_lr_model.linear_layer.out_features\n",
        "\n",
        "print(f\"Number of Input Features: {in_features}\")\n",
        "print(f\"Number of Output Features: {out_features}\")\n",
        "print(f\"Expected Number of Input Features: {d_input}\")\n",
        "print(f\"Expected Number of Output Features: {1}\")\n",
        "assert in_features == d_input and out_features == 1\n",
        "\n",
        "print(\"**********************************\\n\")\n",
        "d_input = 24\n",
        "sample_lr_model = LogisticRegressionModel(d_input = d_input)\n",
        "print(f\"Sample Test Case 2: Testing linear layer input and output sizes, for d_input = {d_input}\")\n",
        "in_features = sample_lr_model.linear_layer.in_features\n",
        "out_features = sample_lr_model.linear_layer.out_features\n",
        "\n",
        "print(f\"Number of Input Features: {in_features}\")\n",
        "print(f\"Number of Output Features: {out_features}\")\n",
        "print(f\"Expected Number of Input Features: {d_input}\")\n",
        "print(f\"Expected Number of Output Features: {1}\")\n",
        "assert in_features == d_input and out_features == 1\n",
        "print(\"**********************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 3: Checking if the model gives correct output\")\n",
        "test_input = torch.rand(d_input)\n",
        "model_output = sample_lr_model(test_input)\n",
        "model_output_np = model_output.detach().numpy()\n",
        "expected_output = 0.6298196315765381\n",
        "print(f\"Model Output: {model_output_np}\")\n",
        "print(f\"Expected Output: {expected_output}\")\n",
        "\n",
        "assert np.allclose(model_output_np, expected_output, 1e-5)\n",
        "print(\"**********************************\\n\")\n",
        "\n",
        "print(f\"Sample Test Case 4: Checking if the model gives correct output\")\n",
        "test_input = torch.rand(4, d_input)\n",
        "model_output = sample_lr_model(test_input)\n",
        "model_output_np = model_output.detach().numpy()\n",
        "expected_output = np.array([0.5503339, 0.5428218, 0.561816,  0.51846  ])\n",
        "print(f\"Model Output: {model_output_np}\")\n",
        "print(f\"Expected Output: {expected_output}\")\n",
        "\n",
        "assert model_output_np.shape == expected_output.shape and np.allclose(model_output_np, expected_output, 1e-5)\n",
        "print(\"**********************************\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d587fc76",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "6f18cdd59636e10a2ac405f717b20d26",
          "grade": false,
          "grade_id": "cell-4ab15e4378615a15",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "d587fc76"
      },
      "source": [
        "### Task 2.2: Training and Evaluating the Model (5 Marks)\n",
        "\n",
        "Write the training and evaluation script like the last time to train and evaluate sentiment classification model. You will need to write the entire functions on your own this time. You can refer to the code in Assignment 1."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03f106f5",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "41f14982553ba28af7c8d6e8797c65e3",
          "grade": true,
          "grade_id": "cell-b8cd46cbe3da08ec",
          "locked": false,
          "points": 2.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "03f106f5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.optim import Adam\n",
        "\n",
        "def train(model, train_dataloader,\n",
        "          lr = 1e-3, num_epochs = 20,\n",
        "          device = \"cpu\"):\n",
        "\n",
        "    \"\"\"\n",
        "    Runs the training loop. Define the loss function as BCELoss like the last tine\n",
        "    and optimizer as Adam and traine for `num_epochs` epochs.\n",
        "\n",
        "    Inputs:\n",
        "        - model (LogisticRegressionModel): A classifer model to be trained\n",
        "        - train_dataloader (torch.utils.DataLoader): A dataloader defined over the training dataset\n",
        "        - lr (float): The learning rate for the optimizer\n",
        "        - num_epochs (int): Number of epochs to train the model for.\n",
        "        - device (str): Device to train the model on. Can be either 'cuda' (for using gpu) or 'cpu'\n",
        "\n",
        "    Returns:\n",
        "        - model (LogisticRegressionModel): Model after completing the training\n",
        "        - epoch_loss (float) : Loss value corresponding to the final epoch\n",
        "    \"\"\"\n",
        "    # Transfer the model to specified device\n",
        "    model = model.to(device)\n",
        "\n",
        "    # Step 1: Define the Binary Cross Entropy loss function\n",
        "    loss_fn = nn.BCELoss()\n",
        "  \n",
        "    # Step 2: Define Adam Optimizer\n",
        "    optimizer = Adam(model.parameters(),lr=lr)\n",
        "\n",
        "\n",
        "    # Iterate over `num_epochs`\n",
        "    for epoch in range(num_epochs):\n",
        "        epoch_loss = 0 # We can use this to keep track of how the loss value changes as we train the model.\n",
        "        # Iterate over each batch using the `train_dataloader`\n",
        "        for train_batch in tqdm.tqdm(train_dataloader):\n",
        "            # Zero out any gradients stored in the previous steps\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Unwrap the batch to get features and labels\n",
        "            features, labels = train_batch\n",
        "\n",
        "            # Most nn modules and loss functions assume the inputs are of type Float, so convert both features and labels to floats\n",
        "            features = features.float()\n",
        "            labels = labels.float()\n",
        "\n",
        "            # Transfer the features and labels to device\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "\n",
        "            # Step 3: Feed the input features to the model to get predictions\n",
        "            preds = model(features)\n",
        "            # Step 4: Compute the loss and perform backward pass\n",
        "            loss = loss_fn(preds,labels)\n",
        "            loss.backward()\n",
        "\n",
        "            # Step 5: Take optimizer step\n",
        "            optimizer.step()\n",
        "            # Store loss value for tracking\n",
        "            epoch_loss += loss.item()\n",
        "\n",
        "        epoch_loss = epoch_loss / len(train_dataloader)\n",
        "        print(f\"Epoch {epoch} completed.. Average Loss: {epoch_loss}\")\n",
        "\n",
        "    return model, epoch_loss\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dae507c4",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "6b6ece51ecf913ab8d9162d1d4f922a8",
          "grade": true,
          "grade_id": "cell-69ff1eb67c532051",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dae507c4",
        "outputId": "ac0eb08c-1887-4d99-cc63-980025ca96b7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training on 100 data points for sanity check\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 12.24it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed.. Average Loss: 0.6949431896209717\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 282.25it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed.. Average Loss: 0.6701762676239014\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 164.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed.. Average Loss: 0.6490162014961243\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 166.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed.. Average Loss: 0.6294075548648834\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 181.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed.. Average Loss: 0.6111842691898346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 164.13it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed.. Average Loss: 0.5942146182060242\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 391.68it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed.. Average Loss: 0.5783675312995911\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 184.94it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed.. Average Loss: 0.5635259449481964\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 435.27it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed.. Average Loss: 0.5495925843715668\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 576.14it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 completed.. Average Loss: 0.536488264799118\n",
            "Final Loss Value: 0.536488264799118\n",
            "Expected Loss Value: 0.5364882349967957\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(42)\n",
        "print(\"Training on 100 data points for sanity check\")\n",
        "sample_documents = train_df[\"sentence\"].values.tolist()[:100]\n",
        "sample_labels = train_df[\"label\"].values.tolist()[:100]\n",
        "sample_features = np.array([get_bow_sent_vec(document, wv) for document in sample_documents])\n",
        "sample_dataset = SST2Dataset(sample_features, sample_labels)\n",
        "sample_dataloader = DataLoader(sample_dataset, batch_size=64)\n",
        "sample_lr_model = LogisticRegressionModel(d_input = len(sample_features[0]))\n",
        "\n",
        "sample_lr_model, loss = train(sample_lr_model, sample_dataloader,\n",
        "      lr = 1e-2, num_epochs = 10,\n",
        "      device = \"cpu\")\n",
        "\n",
        "expected_loss = 0.5364882349967957\n",
        "print(f\"Final Loss Value: {loss}\")\n",
        "print(f\"Expected Loss Value: {expected_loss}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0eade345",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d442bd0dd0ae8b68705825068dae5aed",
          "grade": false,
          "grade_id": "cell-e384402d5adfb651",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "0eade345"
      },
      "source": [
        "Don't worry if the loss values do not match exactly but you should see a decreasing trend and the final value should be of the same order of magnitude"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "452efdb4",
      "metadata": {
        "deletable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "90af5302d037d7e8f4bc0a49ac738737",
          "grade": true,
          "grade_id": "cell-a6e60ff7e33612d0",
          "locked": false,
          "points": 2.5,
          "schema_version": 3,
          "solution": true,
          "task": false
        },
        "id": "452efdb4"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "def evaluate(model, test_dataloader, threshold = 0.5, device = \"cpu\"):\n",
        "    \n",
        "    \"\"\"\n",
        "    Evaluates `model` on test dataset\n",
        "\n",
        "    Inputs:\n",
        "        - model (LogisticRegressionModel or MLPModel): Logistic Regression model to be evaluated\n",
        "        - test_dataloader (torch.utils.DataLoader): A dataloader defined over the test dataset\n",
        "\n",
        "    Returns:\n",
        "        - accuracy (float): Average accuracy over the test dataset \n",
        "    \"\"\"    \n",
        "\n",
        "    model.to(device)\n",
        "    model = model.eval() # Set model to evaluation model \n",
        "    accuracy = 0\n",
        "    \n",
        "    # by specifying `torch.no_grad`, it ensures no gradients are calcuated while running the model,\n",
        "    # this makes the computation much more faster\n",
        "    with torch.no_grad():\n",
        "      for test_batch in test_dataloader:\n",
        "        features, labels = test_batch\n",
        "        features = features.float().to(device)\n",
        "        labels = labels.float().to(device)\n",
        "\n",
        "        # Step 1: Get probability predictions from the model and store it in `pred_probs`\n",
        "        pred_probs = model(features)\n",
        "\n",
        "        # Convert predictions and labels to numpy arrays from torch tensors as they are easier to operate for computing metrics\n",
        "        pred_probs = pred_probs.detach().cpu().numpy()\n",
        "        labels = labels.detach().cpu().numpy()\n",
        "\n",
        "        # Step 2: Get accuracy of predictions and store it in `batch_accuracy`\n",
        "        predictions =[]\n",
        "        for value in pred_probs:\n",
        "          if value>threshold:\n",
        "            predictions.append(1)\n",
        "          else:\n",
        "            predictions.append(0)\n",
        "\n",
        "\n",
        "        batch_accuracy = None\n",
        "        correct_predictions=0\n",
        "        for prediction,actual in zip(predictions,labels):\n",
        "          if prediction==actual:\n",
        "            correct_predictions+=1\n",
        "        batch_accuracy=correct_predictions/len(predictions)\n",
        "\n",
        "\n",
        "        accuracy += batch_accuracy\n",
        "\n",
        "      # Divide by number of batches to get average accuracy\n",
        "      accuracy = accuracy / len(test_dataloader)\n",
        "\n",
        "      return accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "410b1c36",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "673730e41b6beeca5ca4153e3411722e",
          "grade": true,
          "grade_id": "cell-2639c9ccaacbd8df",
          "locked": true,
          "points": 0,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "410b1c36",
        "outputId": "677a02de-ee72-4ff5-b3da-4f5b392468cd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing the sample model on 100 examples for sanity check\n",
            "Accuracy: 0.7204861111111112\n",
            "Expected Accuracy: 0.7204861111111112\n"
          ]
        }
      ],
      "source": [
        "print(f\"Testing the sample model on 100 examples for sanity check\")\n",
        "torch.manual_seed(42)\n",
        "sample_documents = test_df[\"sentence\"].values.tolist()[:100]\n",
        "sample_labels = test_df[\"label\"].values.tolist()[:100]\n",
        "sample_features = np.array([get_bow_sent_vec(document, wv) for document in sample_documents])\n",
        "\n",
        "sample_dataset = SST2Dataset(sample_features,\n",
        "                            sample_labels)\n",
        "\n",
        "sample_dataloader = DataLoader(sample_dataset, batch_size = 64)\n",
        "accuracy = evaluate(sample_lr_model, sample_dataloader, device =\"cpu\")\n",
        "expected_accuracy = 0.7204861111111112\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Expected Accuracy: {expected_accuracy}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f8dc1f3",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "d245f1493f48683bc87ef9f9f482277d",
          "grade": false,
          "grade_id": "cell-460ebcb1a5f355f8",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "5f8dc1f3"
      },
      "source": [
        "Now that you have implemented the training and evaluation functions, we will train (and evaluate) 2 different models and compare their performance. The 2 models are:\n",
        "\n",
        "    - Logistic Regression with Bag of Word2vec features\n",
        "    - Logistic Regression with Weighted Bag of Word2vec features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1a9b6910",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "code",
          "checksum": "e8166a37c5dc46a3bd8a97d146e74deb",
          "grade": false,
          "grade_id": "cell-77b3ce9c9547870f",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1a9b6910",
        "outputId": "721068a1-cd73-4e7d-fc36-5233bcb296bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and Evaluating Logistic Regression with Bag of Word2vec features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:02<00:00, 524.34it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed.. Average Loss: 0.40442544707262507\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 547.97it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed.. Average Loss: 0.37194564222497023\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 568.37it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed.. Average Loss: 0.3695470960306646\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 556.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed.. Average Loss: 0.36882112299048775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 571.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed.. Average Loss: 0.3685299224182185\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 571.39it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed.. Average Loss: 0.36839686528003907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 551.26it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed.. Average Loss: 0.3683308569567609\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 541.46it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed.. Average Loss: 0.3682959823668399\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 555.80it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed.. Average Loss: 0.3682767879447819\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 564.78it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 completed.. Average Loss: 0.36826538582529666\n",
            "Test Accuracy: 0.8087053571428572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training and Evaluating Logistic Regression with Bag of Word2vec features\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_labels = train_df[\"label\"].values.tolist()\n",
        "test_labels = test_df[\"label\"].values.tolist()\n",
        "\n",
        "train_dataset = SST2Dataset(train_bow_vectors, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 64)\n",
        "\n",
        "test_dataset = SST2Dataset(test_bow_vectors, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 64)\n",
        "\n",
        "lr_bow_model = LogisticRegressionModel(\n",
        "    d_input = wv.vector_size,\n",
        ")\n",
        "\n",
        "lr_bow_model, loss = train(lr_bow_model, train_loader,\n",
        "      lr = 1e-2, num_epochs = 10,\n",
        "      device = device)\n",
        "\n",
        "test_accuracy = evaluate(\n",
        "    lr_bow_model, test_loader,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5243ec3d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5243ec3d",
        "outputId": "cd2e8172-1427-4ddd-dbf4-84cd3be821ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training and Evaluating Logistic Regression with Weighted Bag of Word2vec features\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 544.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 completed.. Average Loss: 0.4211177029925534\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 554.67it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 completed.. Average Loss: 0.3881261649153285\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 554.81it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2 completed.. Average Loss: 0.385535904173611\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 553.38it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 3 completed.. Average Loss: 0.3847372835999088\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 557.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 4 completed.. Average Loss: 0.3844115064609424\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 548.44it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 5 completed.. Average Loss: 0.3842606441615767\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 555.56it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 6 completed.. Average Loss: 0.3841837417309554\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 550.96it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 7 completed.. Average Loss: 0.38414362674368524\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 565.55it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 8 completed.. Average Loss: 0.38411985124051856\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1053/1053 [00:01<00:00, 544.34it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 9 completed.. Average Loss: 0.38410562988643066\n",
            "Test Accuracy: 0.7933035714285713\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "print(f\"Training and Evaluating Logistic Regression with Weighted Bag of Word2vec features\")\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "train_labels = train_df[\"label\"].values.tolist()\n",
        "test_labels = test_df[\"label\"].values.tolist()\n",
        "\n",
        "train_dataset = SST2Dataset(train_w_bow_vectors, train_labels)\n",
        "train_loader = DataLoader(train_dataset, batch_size = 64)\n",
        "\n",
        "test_dataset = SST2Dataset(test_w_bow_vectors, test_labels)\n",
        "test_loader = DataLoader(test_dataset, batch_size = 64)\n",
        "\n",
        "lr_bow_model = LogisticRegressionModel(\n",
        "    d_input = wv.vector_size,\n",
        ")\n",
        "\n",
        "lr_bow_model, loss = train(lr_bow_model, train_loader,\n",
        "      lr = 1e-2, num_epochs = 10,\n",
        "      device = device)\n",
        "\n",
        "test_accuracy = evaluate(\n",
        "    lr_bow_model, test_loader,\n",
        "    device = device\n",
        ")\n",
        "\n",
        "print(f\"Test Accuracy: {test_accuracy}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "059a85bb",
      "metadata": {
        "deletable": false,
        "editable": false,
        "nbgrader": {
          "cell_type": "markdown",
          "checksum": "57335f185f6b61b30194a933c4b84f29",
          "grade": false,
          "grade_id": "cell-3feb895a07064c4d",
          "locked": true,
          "schema_version": 3,
          "solution": false,
          "task": false
        },
        "id": "059a85bb"
      },
      "source": [
        "First thing that you can notice is that these models train substantially faster than the models in Assignment 1, as now we have much more lower sized sentence representations i.e. 300, compared to last time when it was equal to the size of vocabulary i.e. around 10k!\n",
        "\n",
        "Both models get around ~80% test accuracy, which is close to what we got with Bag of Words features in Assignment 1 only. The reason we do not see much improvement in performance is because both models still take a (weighted) sum of the individual word vectors to obtain sentence vectors, and fails to encode any structural information as well as semantics properly. For eg. both of the following sentences:\n",
        "\n",
        "- it was a good movie adapted from a bad book\n",
        "- it was a bad movie adapted from a good book\n",
        "\n",
        "both of these sentences will get exact similar vector representations according to both the methods and hence the model will never be able to distinguish between the sentiment of these two sentences giving same prediction for both. \n",
        "\n",
        "In the next assignments we shall see how we can learn more contextual representation of the sentences that can help us solve the task much more efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc0ab094",
      "metadata": {
        "id": "fc0ab094"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "colab": {
      "name": "Assignment2b.ipynb",
      "provenance": []
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}